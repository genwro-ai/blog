@inproceedings{lime,
    author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
    title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
    year = {2016},
    isbn = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2939672.2939778},
    doi = {10.1145/2939672.2939778},
    keywords = {interpretable machine learning, interpretability, explaining machine learning, black box classifier},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}

@inproceedings{shap,
    author = {Lundberg, Scott and Lee, Su-In},
    year = {2017},
    month = {12},
    pages = {},
    title = {A Unified Approach to Interpreting Model Predictions},
    doi = {10.48550/arXiv.1705.07874}
}

@article{manifesto,
    title = {Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions},
    journal = {Information Fusion},
    volume = {106},
    pages = {102301},
    year = {2024},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2024.102301},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253524000794},
    author = {Luca Longo and Mario Brcic and Federico Cabitza and Jaesik Choi and Roberto Confalonieri and Javier Del Ser and Riccardo Guidotti and Yoichi Hayashi and Francisco Herrera and Andreas Holzinger and Richard Jiang and Hassan Khosravi and Freddy Lecue and Gianclaudio Malgieri and Andrés Páez and Wojciech Samek and Johannes Schneider and Timo Speith and Simone Stumpf},
    keywords = {Explainable artificial intelligence, XAI, Interpretability, Manifesto, Open challenges, Interdisciplinarity, Ethical AI, Large language models, Trustworthy AI, Responsible AI, Generative AI, Multi-faceted explanations, Concept-based explanations, Causality, Actionable XAI, Falsifiability},
    abstract = {Understanding black box models has become paramount as systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper highlights the advancements in XAI and its application in real-world scenarios and addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. We aim to develop a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 28 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.}
}

@article{survey,
    title = {Explainable artificial intelligence: A survey of needs, techniques, applications, and future direction},
    journal = {Neurocomputing},
    volume = {599},
    pages = {128111},
    year = {2024},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2024.128111},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231224008828},
    author = {Melkamu Mersha and Khang Lam and Joseph Wood and Ali K. AlShami and Jugal Kalita},
    keywords = {XAI, Explainable artificial intelligence, Interpretable deep learning, Machine learning, Neural networks, Evaluation methods, Computer vision, Natural language processing, NLP, Transformers, Time series, Healthcare, Autonomous cars},
    abstract = {Artificial intelligence models encounter significant challenges due to their black-box nature, particularly in safety-critical domains such as healthcare, finance, and autonomous vehicles. Explainable Artificial Intelligence (XAI) addresses these challenges by providing explanations for how these models make decisions and predictions, ensuring transparency, accountability, and fairness. Existing studies have examined the fundamental concepts of XAI, its general principles, and the scope of XAI techniques. However, there remains a gap in the literature as there are no comprehensive reviews that delve into the detailed mathematical representations, design methodologies of XAI models, and other associated aspects. This paper provides a comprehensive literature review encompassing common terminologies and definitions, the need for XAI, beneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI methods in different application areas. The survey is aimed at XAI researchers, XAI practitioners, AI model developers, and XAI beneficiaries who are interested in enhancing the trustworthiness, transparency, accountability, and fairness of their AI models.}
}

@article{Guidotti2022,
  title = {Counterfactual explanations and how to find them: literature review and benchmarking},
  volume = {38},
  ISSN = {1573-756X},
  url = {http://dx.doi.org/10.1007/s10618-022-00831-6},
  DOI = {10.1007/s10618-022-00831-6},
  number = {5},
  journal = {Data Mining and Knowledge Discovery},
  publisher = {Springer Science and Business Media LLC},
  author = {Guidotti,  Riccardo},
  year = {2022},
  month = apr,
  pages = {2770–2824}
}

@article{Wachter2017,
  title={Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  author={Sandra Wachter and Brent Daniel Mittelstadt and Chris Russell},
  journal={Cybersecurity},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:3995299}
}